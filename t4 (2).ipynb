{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjZhiDJiyVIO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e82V6ooLrauj",
    "outputId": "fad338a1-19a0-4a5f-d53e-40debb67b82d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar to D:/DATASETS/segmentation/VOCtrainval_11-May-2012.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2.00G/2.00G [11:23<00:00, 2.93MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting D:/DATASETS/segmentation/VOCtrainval_11-May-2012.tar to D:/DATASETS/segmentation\n",
      "Training samples: 2184, Validation samples: 729\n",
      "torch.Size([10, 3, 128, 128]) torch.Size([10, 1, 128, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|█| 219/219 [57:08<00:00, 15.65s/batch, acc=0.799, dice=0.0221, iou\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.231534601210459, Train Acc: 0.7993181383399715, Train IoU: 0.0123526780857726, Train Dice: 0.022139836210020716, Val Loss: 0.2604102039173858, Val Acc: 0.9473016147744165, Val IoU: 2.198041404900496e-09, Val Dice: 2.198041404900496e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|█| 219/219 [41:28<00:00, 11.36s/batch, acc=0.835, dice=0.0461, iou\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.8118846572289183, Train Acc: 0.8348645529790556, Train IoU: 0.026451331100296675, Train Dice: 0.046079900666448105, Val Loss: 0.22300300471586723, Val Acc: 0.9473016147744165, Val IoU: 2.198041404900496e-09, Val Dice: 2.198041404900496e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|█| 219/219 [10:09:27<00:00, 166.98s/batch, acc=0.86, dice=0.0652, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.7973634341399963, Train Acc: 0.8600234032765915, Train IoU: 0.03726623050743221, Train Dice: 0.0651566607902506, Val Loss: 0.2119305962569093, Val Acc: 0.9473016147744165, Val IoU: 2.198041404900496e-09, Val Dice: 2.198041404900496e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  99%|▉| 217/219 [2:41:59<00:22, 11.36s/batch, acc=0.854, dice=0.069, io"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# =============================\n",
    "# Define Variables and Configs\n",
    "# =============================\n",
    "NUM_CLASSES = 21\n",
    "BATCH_SIZE = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 50\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PATIENCE = 7  # Early stopping patience\n",
    "MIN_LR = 1e-6  # Minimum learning rate for ReduceLROnPlateau\n",
    "IMG_SIZE = 128  # Ensure this is divisible by 2^n\n",
    "\n",
    "# =========================\n",
    "# Data Transforms and Setup\n",
    "# =========================\n",
    "input_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load Datasets\n",
    "data = VOCSegmentation(root=\"D:/DATASETS/segmentation\", year=\"2012\", image_set=\"trainval\", download=True,\n",
    "                                 transform=input_transform, target_transform=target_transform)\n",
    "train_dataset,val_dataset=torch.utils.data.random_split(data,[int(len(data)*0.75),round(len(data)*0.25)+1])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=3)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=3)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "data_iter = iter(train_loader)\n",
    "images, masks = next(data_iter)\n",
    "print(images.shape, masks.shape)\n",
    "\n",
    "# ================================\n",
    "# Define U-Net++ Model with Dropout\n",
    "# ================================\n",
    "class UNetPlusPlus(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, base_channels=64, dropout_rate=0.3):\n",
    "        super(UNetPlusPlus, self).__init__()\n",
    "\n",
    "        # Encoder (Contracting path)\n",
    "        self.encoder1 = self.conv_block(n_channels, base_channels)\n",
    "        self.encoder2 = self.conv_block(base_channels, base_channels * 2)\n",
    "        self.encoder3 = self.conv_block(base_channels * 2, base_channels * 4)\n",
    "        self.encoder4 = self.conv_block(base_channels * 4, base_channels * 8)\n",
    "\n",
    "        # Decoder (Expanding path)\n",
    "        self.decoder1 = self.upconv_block(base_channels * 8, base_channels * 4)\n",
    "        self.decoder2 = self.upconv_block(base_channels * 4, base_channels * 2)\n",
    "        self.decoder3 = self.upconv_block(base_channels * 2, base_channels)\n",
    "        self.decoder4 = nn.ConvTranspose2d(base_channels, n_classes, kernel_size=2, stride=2)\n",
    "\n",
    "        # Nested Skip Connections\n",
    "        self.decoder1_1 = self.upconv_block(base_channels * 8 + base_channels * 4, base_channels * 4)\n",
    "        self.decoder2_1 = self.upconv_block(base_channels * 4 + base_channels * 2, base_channels * 2)\n",
    "        self.decoder3_1 = self.upconv_block(base_channels * 2 + base_channels, base_channels)\n",
    "\n",
    "        self.dropout = nn.Dropout2d(dropout_rate)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        \"\"\"Convolution block with Batch Normalization and ReLU.\"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def upconv_block(self, in_channels, out_channels):\n",
    "        \"\"\"Upsampling block with ConvTranspose and Batch Normalization.\"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder Pathway\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(e1)\n",
    "        e3 = self.encoder3(e2)\n",
    "        e4 = self.encoder4(e3)\n",
    "\n",
    "        # Decoder Pathway\n",
    "        d1 = self.decoder1(e4)\n",
    "        d1 = nn.functional.interpolate(d1, size=e3.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "        d2 = self.decoder2(d1)\n",
    "        d2 = nn.functional.interpolate(d2, size=e2.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "        d3 = self.decoder3(d2)\n",
    "        d3 = nn.functional.interpolate(d3, size=e1.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "        out = self.decoder4(d3)\n",
    "\n",
    "        # Nested Skip Pathways\n",
    "        d1_1 = self.decoder1_1(torch.cat([e4, d1], 1))\n",
    "        d2_1 = self.decoder2_1(torch.cat([e3, d2], 1))\n",
    "        d3_1 = self.decoder3_1(torch.cat([e2, d3], 1))\n",
    "\n",
    "        # Project intermediate tensors to NUM_CLASSES channels\n",
    "        device = x.device  # Get the device of the input\n",
    "        proj_d1_1 = nn.Conv2d(d1_1.shape[1], NUM_CLASSES, kernel_size=1).to(device)(d1_1)\n",
    "        proj_d2_1 = nn.Conv2d(d2_1.shape[1], NUM_CLASSES, kernel_size=1).to(device)(d2_1)\n",
    "        proj_d3_1 = nn.Conv2d(d3_1.shape[1], NUM_CLASSES, kernel_size=1).to(device)(d3_1)\n",
    "\n",
    "        # Combine outputs\n",
    "        out = out + proj_d1_1 + proj_d2_1 + proj_d3_1\n",
    "        out = self.dropout(out)\n",
    "        out = nn.functional.interpolate(out, size=(x.shape[2], x.shape[3]), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "model = UNetPlusPlus(n_channels=3, n_classes=NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# ==============================\n",
    "# Define Loss, Optimizer, Scheduler\n",
    "# ==============================\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)  # Ignore unlabeled pixels\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=3, factor=0.9, min_lr=MIN_LR)\n",
    "\n",
    "# ============================\n",
    "# Training Loop\n",
    "# ============================\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=1e-4, mode=\"min\"):\n",
    "        \"\"\"\n",
    "        Early stopping to terminate training when validation loss stops improving.\n",
    "        :param patience: How many epochs to wait after last improvement.\n",
    "        :param min_delta: Minimum change to qualify as an improvement.\n",
    "        :param mode: \"min\" for loss (lower is better), \"max\" for accuracy/score (higher is better).\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.best_score = None\n",
    "        self.epochs_no_improve = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, current_score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "        else:\n",
    "            improvement = (current_score < self.best_score - self.min_delta) if self.mode == \"min\" else \\\n",
    "                          (current_score > self.best_score + self.min_delta)\n",
    "            if improvement:\n",
    "                self.best_score = current_score\n",
    "                self.epochs_no_improve = 0\n",
    "            else:\n",
    "                self.epochs_no_improve += 1\n",
    "                if self.epochs_no_improve >= self.patience:\n",
    "                    self.early_stop = True\n",
    "\n",
    "# ============================\n",
    "# Utility Function for Accuracy\n",
    "# ============================\n",
    "def calculate_accuracy(outputs, masks):\n",
    "    \"\"\"Calculate pixel-wise accuracy.\"\"\"\n",
    "    # Get the predicted class for each pixel\n",
    "    preds = torch.argmax(outputs, dim=1)  # Shape: (batch_size, height, width)\n",
    "    correct = (preds == masks).float()  # Pixel-wise comparison\n",
    "    accuracy = correct.sum() / correct.numel()  # Total correct pixels / total pixels\n",
    "    return accuracy.item()\n",
    "# ============================\n",
    "# Utility Functions for Metrics\n",
    "# ============================\n",
    "def calculate_iou(outputs, masks):\n",
    "    \"\"\"Calculate Intersection over Union (IoU).\"\"\"\n",
    "    preds = torch.argmax(outputs, dim=1)  # Shape: (batch_size, height, width)\n",
    "    masks = masks.squeeze(1)  # Remove channel dimension if present\n",
    "    intersection = (preds & masks).float().sum((1, 2))  # Logical AND\n",
    "    union = (preds | masks).float().sum((1, 2))  # Logical OR\n",
    "    iou = (intersection + 1e-6) / (union + 1e-6)  # Avoid division by zero\n",
    "    return iou.mean().item()\n",
    "\n",
    "def calculate_dice(outputs, masks):\n",
    "    \"\"\"Calculate Dice Score.\"\"\"\n",
    "    preds = torch.argmax(outputs, dim=1)  # Shape: (batch_size, height, width)\n",
    "    masks = masks.squeeze(1)  # Remove channel dimension if present\n",
    "    intersection = (preds & masks).float().sum((1, 2))  # Logical AND\n",
    "    dice = (2.0 * intersection + 1e-6) / (preds.float().sum((1, 2)) + masks.float().sum((1, 2)) + 1e-6)\n",
    "    return dice.mean().item()\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Training Loop with IoU and Dice\n",
    "# ============================\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": [], \"train_iou\": [], \"val_iou\": [], \"train_dice\": [], \"val_dice\": []}\n",
    "early_stopping = EarlyStopping(patience=7, min_delta=1e-4, mode=\"min\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    train_iou = 0\n",
    "    train_dice = 0\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        for images, masks in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch + 1}\")\n",
    "            images, masks = images.to(DEVICE), masks.long().to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            train_iou += calculate_iou(outputs, masks)\n",
    "            train_dice += calculate_dice(outputs, masks)\n",
    "            loss = criterion(outputs, masks.squeeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_acc += calculate_accuracy(outputs, masks)\n",
    "            train_iou += calculate_iou(outputs, masks)\n",
    "            train_dice += calculate_dice(outputs, masks)\n",
    "\n",
    "            tepoch.set_postfix(\n",
    "                loss=train_loss / len(train_loader),\n",
    "                acc=train_acc / len(train_loader),\n",
    "                iou=train_iou / len(train_loader),\n",
    "                dice=train_dice / len(train_loader)\n",
    "            )\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss / len(train_loader))\n",
    "    history[\"train_acc\"].append(train_acc / len(train_loader))\n",
    "    history[\"train_iou\"].append(train_iou / len(train_loader))\n",
    "    history[\"train_dice\"].append(train_dice / len(train_loader))\n",
    "\n",
    "    # Validation Step\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    val_iou = 0\n",
    "    val_dice = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images, masks = images.to(DEVICE), masks.long().to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks.squeeze(1))\n",
    "            val_loss += loss.item()\n",
    "            val_acc += calculate_accuracy(outputs, masks)\n",
    "            val_iou += calculate_iou(outputs, masks)\n",
    "            val_dice += calculate_dice(outputs, masks)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc /= len(val_loader)\n",
    "    val_iou /= len(val_loader)\n",
    "    val_dice /= len(val_loader)\n",
    "\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    history[\"val_iou\"].append(val_iou)\n",
    "    history[\"val_dice\"].append(val_dice)\n",
    "\n",
    "    # Print metrics and apply scheduler\n",
    "    print(f\"Epoch {epoch + 1}: Train Loss: {history['train_loss'][-1]}, Train Acc: {history['train_acc'][-1]}, \"\n",
    "          f\"Train IoU: {history['train_iou'][-1]}, Train Dice: {history['train_dice'][-1]}, \"\n",
    "          f\"Val Loss: {history['val_loss'][-1]}, Val Acc: {history['val_acc'][-1]}, \"\n",
    "          f\"Val IoU: {history['val_iou'][-1]}, Val Dice: {history['val_dice'][-1]}\")\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    early_stopping(val_loss)  # Use validation loss or dice as the monitored metric\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
